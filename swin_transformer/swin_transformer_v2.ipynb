{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":65626,"databundleVersionId":8046133,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Goal:\n- Simplify swin transformer model with comprehensive setup for train, val, test with metrics (loss, r2 for log, r2 for original, mae, etc.)\n    1. Do not use log transformation for trait targets (may have high variance in outputs) and continue to use R2 loss\n    2. Use log transform for trait targets & adjust R2 loss to be calculated in original scale (not log-scale) OR add a layer of exp10 (10^x) in model for backprop\n\n- [Later] Include hyperparameter tuning framework, add visualizations of first layer","metadata":{}},{"cell_type":"markdown","source":"Credits:\n- Modified from HDJOJO's original notebook with SWIN Transformer, which was modified from https://www.kaggle.com/code/markwijkhuizen/planttraits2024-eda-training-pub.\n- Training only, EDA part not included.\n- Image model only, tabular data not used.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport imageio.v3 as imageio\nimport albumentations as A\n\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import StandardScaler\n\nimport torch\nimport timm\n# import glob\nimport torchmetrics\nimport time\nimport psutil\nimport os\n\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T07:09:17.216662Z","iopub.execute_input":"2024-06-01T07:09:17.217084Z","iopub.status.idle":"2024-06-01T07:09:17.224532Z","shell.execute_reply.started":"2024-06-01T07:09:17.217054Z","shell.execute_reply":"2024-06-01T07:09:17.223429Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Config():\n    IMAGE_SIZE = 256\n#     BACKBONE = 'swin_large_patch4_window12_384.ms_in22k_ft_in1k'\n    BACKBONE = 'swinv2_small_window16_256'\n    TARGET_COLUMNS = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n    N_TARGETS = len(TARGET_COLUMNS)\n    BATCH_SIZE = 128\n    LR_MAX = 1e-4\n    WEIGHT_DECAY = 0.01\n    N_EPOCHS = 10\n    TRAIN_MODEL = True\n    IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n    \n    # Added variables\n    NUM_FOLDS = 5\n    VALID_FOLD = 0  # Fold of validation data\n        \nCONFIG = Config()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T07:09:18.672391Z","iopub.execute_input":"2024-06-01T07:09:18.673200Z","iopub.status.idle":"2024-06-01T07:09:18.680385Z","shell.execute_reply.started":"2024-06-01T07:09:18.673164Z","shell.execute_reply":"2024-06-01T07:09:18.679059Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Read in training data\ntrain_df = pd.read_csv('/kaggle/input/planttraits2024/train.csv')\ntrain_df['file_path'] = train_df['id'].apply(lambda s: f'/kaggle/input/planttraits2024/train_images/{s}.jpeg')\ntrain_df['jpeg_bytes'] = train_df['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\ntrain_df.to_pickle('train.pkl') # serialize object into string form\nprint(\"Train df length:\", len(train_df))","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:24:37.493568Z","iopub.execute_input":"2024-05-31T01:24:37.493994Z","iopub.status.idle":"2024-05-31T01:29:05.76668Z","shell.execute_reply.started":"2024-05-31T01:24:37.493962Z","shell.execute_reply":"2024-05-31T01:29:05.765373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Filtering","metadata":{}},{"cell_type":"code","source":"# Sampled training set for faster training\nprint(\"Previous length:\", len(train_df))\n# train_df = train_df.sample(frac=0.3, random_state=42)\n# print(\"Sampled length:\", len(train_df))","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:29:05.768689Z","iopub.execute_input":"2024-05-31T01:29:05.76927Z","iopub.status.idle":"2024-05-31T01:29:05.778372Z","shell.execute_reply.started":"2024-05-31T01:29:05.769237Z","shell.execute_reply":"2024-05-31T01:29:05.776836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=CONFIG.NUM_FOLDS, shuffle=True, random_state=42)\n\n# Create separate bin for each traits\nfor i, trait in enumerate(CONFIG.TARGET_COLUMNS):\n    # Determine the bin edges dynamically based on the distribution of traits\n    bin_edges = np.percentile(train_df[trait], np.linspace(0, 100, CONFIG.NUM_FOLDS + 1))\n    train_df[f\"bin_{i}\"] = np.digitize(train_df[trait], bin_edges)\n\n# Concatenate the bins into a final bin\ntrain_df[\"final_bin\"] = (\n    train_df[[f\"bin_{i}\" for i in range(CONFIG.N_TARGETS)]]\n    .astype(str)\n    .agg(\"\".join, axis=1)\n)\n\n# Perform the stratified split using final bin\ntrain_df = train_df.reset_index(drop=True)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df, train_df[\"final_bin\"])):\n    train_df.loc[valid_idx, \"fold\"] = fold\n    \ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:29:05.780627Z","iopub.execute_input":"2024-05-31T01:29:05.781164Z","iopub.status.idle":"2024-05-31T01:29:07.854817Z","shell.execute_reply.started":"2024-05-31T01:29:05.781115Z","shell.execute_reply":"2024-05-31T01:29:07.852907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train_df[train_df[\"fold\"] != CONFIG.VALID_FOLD]\nvalid = train_df[train_df[\"fold\"] == CONFIG.VALID_FOLD] # Fold 0 is validation\ntrain[CONFIG.TARGET_COLUMNS + [\"fold\"]].describe()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:29:07.859781Z","iopub.execute_input":"2024-05-31T01:29:07.860998Z","iopub.status.idle":"2024-05-31T01:29:08.033298Z","shell.execute_reply.started":"2024-05-31T01:29:07.860942Z","shell.execute_reply":"2024-05-31T01:29:08.032327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PlantDataPreProcess:\n    lower_quantile = 0.005\n    upper_quantile = 0.995\n    log_transform = np.log10","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:29:08.034452Z","iopub.execute_input":"2024-05-31T01:29:08.036077Z","iopub.status.idle":"2024-05-31T01:29:09.327907Z","shell.execute_reply.started":"2024-05-31T01:29:08.036036Z","shell.execute_reply":"2024-05-31T01:29:09.326693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter data\nprint(\"Num samples before filtering:\", len(train))\n\nfor trait in CONFIG.TARGET_COLUMNS:\n    lower_bound = train[trait].quantile(PlantDataPreProcess.lower_quantile)\n    upper_bound = train[trait].quantile(PlantDataPreProcess.upper_quantile)\n    train = train[(train[trait] >= lower_bound) & (train[trait] <= upper_bound)]\n    \nprint(\"Num samples After filtering:\", len(train))\ntrain[CONFIG.TARGET_COLUMNS].describe()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:29:09.329079Z","iopub.execute_input":"2024-05-31T01:29:09.329927Z","iopub.status.idle":"2024-05-31T01:29:11.704548Z","shell.execute_reply.started":"2024-05-31T01:29:09.329895Z","shell.execute_reply":"2024-05-31T01:29:11.702581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Log10 transformation for all traits except X4\nLOG_FEATURES = ['X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\ny_train = train[CONFIG.TARGET_COLUMNS]\n\nfor skewed_trait in LOG_FEATURES:\n    y_train.loc[:, skewed_trait] = y_train[skewed_trait].apply(PlantDataPreProcess.log_transform)\n\ny_train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:29:11.707551Z","iopub.execute_input":"2024-05-31T01:29:11.708341Z","iopub.status.idle":"2024-05-31T01:29:11.785319Z","shell.execute_reply.started":"2024-05-31T01:29:11.708279Z","shell.execute_reply":"2024-05-31T01:29:11.78355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize to mean = 0, std dev = 1\nfrom sklearn.preprocessing import StandardScaler\n\nSCALER = StandardScaler()\ny_train = SCALER.fit_transform(y_train)\n\n# y_train_df = pd.DataFrame(y_train, columns=CONFIG.TARGET_COLUMNS)\n# y_train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:29:11.78787Z","iopub.execute_input":"2024-05-31T01:29:11.78869Z","iopub.status.idle":"2024-05-31T01:29:11.808772Z","shell.execute_reply.started":"2024-05-31T01:29:11.788623Z","shell.execute_reply":"2024-05-31T01:29:11.80655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SWIN Transformer Data Load","metadata":{}},{"cell_type":"code","source":"CONFIG.N_TRAIN_SAMPLES = len(train)\nCONFIG.N_STEPS_PER_EPOCH = (CONFIG.N_TRAIN_SAMPLES // CONFIG.BATCH_SIZE)\nCONFIG.N_STEPS = CONFIG.N_STEPS_PER_EPOCH * CONFIG.N_EPOCHS + 1\n\ntest = pd.read_csv('/kaggle/input/planttraits2024/test.csv')\ntest['file_path'] = test['id'].apply(lambda s: f'/kaggle/input/planttraits2024/test_images/{s}.jpeg')\ntest['jpeg_bytes'] = test['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\ntest.to_pickle('test.pkl')\n\nprint('N_TRAIN_SAMPLES:', len(train), 'N_TEST_SAMPLES:', len(test))","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:29:11.810974Z","iopub.execute_input":"2024-05-31T01:29:11.811489Z","iopub.status.idle":"2024-05-31T01:29:46.396256Z","shell.execute_reply.started":"2024-05-31T01:29:11.811435Z","shell.execute_reply":"2024-05-31T01:29:46.394549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train len:\", len(train))\nprint(\"y_train len\", len(y_train))","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:30:13.141767Z","iopub.execute_input":"2024-05-31T01:30:13.142188Z","iopub.status.idle":"2024-05-31T01:30:13.150703Z","shell.execute_reply.started":"2024-05-31T01:30:13.142156Z","shell.execute_reply":"2024-05-31T01:30:13.148392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\n                CONFIG.BACKBONE,\n                num_classes=CONFIG.N_TARGETS,\n                pretrained=True)  # Use pretrained SWIN Transformer model\n        \n    def forward(self, inputs):\n        return self.backbone(inputs)\n\nmodel = Model()\nmodel = model.to('cuda')\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:30:22.225199Z","iopub.execute_input":"2024-05-31T01:30:22.22565Z","iopub.status.idle":"2024-05-31T01:30:23.243706Z","shell.execute_reply.started":"2024-05-31T01:30:22.225615Z","shell.execute_reply":"2024-05-31T01:30:23.242698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use model-specific image processing (transforms) from 'swinv2_tiny_window16_256' model\n\n# get model specific transforms (normalization, resize)\n# data_config = timm.data.resolve_model_data_config(model)\n# transforms = timm.data.create_transform(**data_config, is_training=False)\n# print(transforms)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:36:26.249517Z","iopub.execute_input":"2024-05-31T01:36:26.249997Z","iopub.status.idle":"2024-05-31T01:36:26.258407Z","shell.execute_reply.started":"2024-05-31T01:36:26.249965Z","shell.execute_reply":"2024-05-31T01:36:26.257163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Where did values come from?\n# Values seem to be the normalization used in training SWIN transformer on image net\nMEAN = np.array([0.485, 0.456, 0.406])\nSTD = np.array([0.229, 0.224, 0.225])\n\nTRAIN_TRANSFORMS = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.RandomSizedCrop(\n            [448, 512],\n            CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE, w2h_ratio=1.0, p=0.75),\n        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.25),\n        A.ImageCompression(quality_lower=85, quality_upper=100, p=0.25),\n        A.ToFloat(),\n        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n        ToTensorV2(),\n    ])\n\nVALID_TRANSFORMS = A.Compose([\n        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n        A.ToFloat(),\n        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n        ToTensorV2(),\n    ])\n\nTEST_TRANSFORMS = A.Compose([\n        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n        A.ToFloat(),\n        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n        ToTensorV2(),\n    ])\n\nclass Dataset(Dataset):\n    def __init__(self, X_jpeg_bytes, y, transforms=None):\n        self.X_jpeg_bytes = X_jpeg_bytes\n        self.y = y\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.X_jpeg_bytes)\n\n    def __getitem__(self, index):\n        X_sample = self.transforms(\n            image=imageio.imread(self.X_jpeg_bytes[index]),\n        )['image']\n        y_sample = self.y[index]\n        \n        return X_sample, y_sample\n\ntrain_dataset = Dataset(\n    train['jpeg_bytes'].values,\n    y_train,\n    TRAIN_TRANSFORMS,\n)\n\ntrain_dataloader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG.BATCH_SIZE,\n        shuffle=True,\n        drop_last=True,\n        num_workers=psutil.cpu_count(),\n)\n\n# Computes validation R2 in the log space (for all traits except X4)\n# bc train R2 is computed in log space\n\nvalid_y = valid[CONFIG.TARGET_COLUMNS].values\n\ndef preprocess_targets(y, scaler, log_features, is_train=True):\n    y = pd.DataFrame(y, columns=CONFIG.TARGET_COLUMNS)\n    if is_train:\n        for skewed_trait in log_features:\n            y[skewed_trait] = y[skewed_trait].apply(PlantDataPreProcess.log_transform)\n        y = scaler.fit_transform(y)\n    else:\n        for skewed_trait in log_features:\n            y[skewed_trait] = y[skewed_trait].apply(PlantDataPreProcess.log_transform)\n        y = scaler.transform(y)\n    return y\n\nvalid_y = preprocess_targets(valid_y, SCALER, LOG_FEATURES, is_train=False)\n\nvalid_dataset = Dataset(\n    valid['jpeg_bytes'].values,\n    valid_y,\n    VALID_TRANSFORMS,\n)\n\nvalid_dataloader = DataLoader(\n        valid_dataset,\n        batch_size=CONFIG.BATCH_SIZE,\n        shuffle=False,\n        num_workers=psutil.cpu_count(),\n)\n\ntest_dataset = Dataset(\n    test['jpeg_bytes'].values,\n    test['id'].values,\n    TEST_TRANSFORMS,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T02:18:35.468179Z","iopub.execute_input":"2024-05-31T02:18:35.468659Z","iopub.status.idle":"2024-05-31T02:18:35.502051Z","shell.execute_reply.started":"2024-05-31T02:18:35.468622Z","shell.execute_reply":"2024-05-31T02:18:35.500612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List available Swin Transformer models in timm library\n# list(filter(lambda x : 'swin' in x, timm.list_models()))","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:56:28.057107Z","iopub.execute_input":"2024-05-31T01:56:28.057569Z","iopub.status.idle":"2024-05-31T01:56:28.063597Z","shell.execute_reply.started":"2024-05-31T01:56:28.057525Z","shell.execute_reply":"2024-05-31T01:56:28.061823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_lr_scheduler(optimizer):\n    return torch.optim.lr_scheduler.OneCycleLR(\n        optimizer=optimizer,\n        max_lr=CONFIG.LR_MAX,\n        total_steps=CONFIG.N_STEPS,\n        pct_start=0.1,\n        anneal_strategy='cos',\n        div_factor=1e1,\n        final_div_factor=1e1,\n    )\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val):\n        self.sum += val.sum()\n        self.count += val.numel()\n        self.avg = self.sum / self.count\n\nMAE = torchmetrics.regression.MeanAbsoluteError().to('cuda')\nR2 = torchmetrics.regression.R2Score(num_outputs=CONFIG.N_TARGETS, multioutput='uniform_average').to('cuda')\nLOSS = AverageMeter()\n\nY_MEAN = torch.tensor(y_train).mean(dim=0).to('cuda')\nEPS = torch.tensor([1e-6]).to('cuda')\n\ndef r2_loss(y_pred, y_true):\n    ss_res = torch.sum((y_true - y_pred)**2, dim=0)\n    ss_total = torch.sum((y_true - Y_MEAN)**2, dim=0)\n    ss_total = torch.maximum(ss_total, EPS)\n    r2 = torch.mean(ss_res / ss_total)\n    return r2\n\n# How is this R2 Loss?\nLOSS_FN = nn.SmoothL1Loss() # r2_loss\n\nlearning_rate = 1e-3\noptimizer = torch.optim.AdamW(\n    params=model.parameters(),\n    #lr=CONFIG.LR_MAX,\n    lr=learning_rate,\n    weight_decay=CONFIG.WEIGHT_DECAY,\n)\n\n# LR_SCHEDULER = get_lr_scheduler(optimizer)\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Define validation loop with batch processing\ndef validate(model, dataloader, scaler, log_features):\n    model.eval()\n    MAE_valid = torchmetrics.MeanAbsoluteError().to('cuda')\n    R2_valid = torchmetrics.R2Score(num_outputs=CONFIG.N_TARGETS, multioutput='uniform_average').to('cuda')\n    losses = []\n    \n    for X_batch, y_true in dataloader:\n        X_batch = X_batch.to('cuda')\n        y_true = y_true.to('cuda')\n        with torch.no_grad():\n            y_pred = model(X_batch)\n            loss = LOSS_FN(y_pred, y_true)\n            losses.append(loss.item())\n            MAE_valid.update(y_pred, y_true)\n            R2_valid.update(y_pred, y_true)\n    \n    valid_r2 = R2_valid.compute().item()\n    valid_mae = MAE_valid.compute().item()\n    valid_loss = np.mean(losses)\n    \n    return valid_r2, valid_mae, valid_loss\n\nmetrics = {\n    'epoch': [],\n    'loss': [],\n    'mae': [],\n    'r2': [],\n    'lr': [],\n    'training_time': [],\n    'num_params': count_parameters(model),\n    'valid_r2': [],\n    'valid_mae': [],\n    'valid_loss': []\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-31T02:19:43.95125Z","iopub.status.idle":"2024-05-31T02:19:43.951778Z","shell.execute_reply.started":"2024-05-31T02:19:43.951526Z","shell.execute_reply":"2024-05-31T02:19:43.951551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_valid_r2 = -np.inf \n\nprint(\"Start Training:\")\nfor epoch in range(CONFIG.N_EPOCHS):\n    epoch_start_time = time.time()\n    MAE.reset()\n    R2.reset()\n    LOSS.reset()\n    model.train()\n    \n    epoch_loss = 0\n    epoch_mae = 0\n    epoch_r2 = 0\n        \n    for step, (X_batch, y_true) in enumerate(train_dataloader):\n        X_batch = X_batch.to('cuda')\n        y_true = y_true.to('cuda')\n        t_start = time.perf_counter_ns()\n        y_pred = model(X_batch)\n        loss = LOSS_FN(y_pred, y_true)\n        LOSS.update(loss)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        #LR_SCHEDULER.step()\n        MAE.update(y_pred, y_true)\n        R2.update(y_pred, y_true)\n        \n        epoch_loss += loss.item()\n        epoch_mae += MAE.compute().item()\n        epoch_r2 += R2.compute().item()\n            \n        if not CONFIG.IS_INTERACTIVE and (step+1) == CONFIG.N_STEPS_PER_EPOCH:\n            print(\n                f'EPOCH {epoch+1:02d}, {step+1:04d}/{CONFIG.N_STEPS_PER_EPOCH} | ' + \n                f'loss: {LOSS.avg:.4f}, mae: {MAE.compute().item():.4f}, r2: {R2.compute().item():.4f}, ' +\n                #f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n                f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {learning_rate:.2e}',\n            )\n        elif CONFIG.IS_INTERACTIVE:\n            print(\n                f'\\rEPOCH {epoch+1:02d}, {step+1:04d}/{CONFIG.N_STEPS_PER_EPOCH} | ' + \n                f'loss: {LOSS.avg:.4f}, mae: {MAE.compute().item():.4f}, r2: {R2.compute().item():.4f}, ' +\n                #f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n                f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {learning_rate:.2e}',\n                end='\\n' if (step + 1) == CONFIG.N_STEPS_PER_EPOCH else '', flush=True,\n            )\n            \n    epoch_training_time = time.time() - epoch_start_time\n    \n    # Validate on validation set\n    valid_r2, valid_mae, valid_loss = validate(model, valid_dataloader, SCALER, LOG_FEATURES)\n    print(\n        f'VALIDATION | epoch: {epoch + 1:02d}, '\n        f'valid_loss: {valid_loss:.4f}, valid_mae: {valid_mae:.4f}, valid_r2: {valid_r2:.4f}'\n    )\n        \n    # Log metrics for this epoch\n    metrics['epoch'].append(epoch + 1)\n    metrics['loss'].append(epoch_loss / len(train_dataloader))\n    metrics['mae'].append(epoch_mae / len(train_dataloader))\n    metrics['r2'].append(epoch_r2 / len(train_dataloader))\n    #metrics['lr'].append(LR_SCHEDULER.get_last_lr()[0])\n    metrics['lr'].append(learning_rate)\n    metrics['training_time'].append(epoch_training_time)\n    metrics['valid_r2'].append(valid_r2)\n    metrics['valid_mae'].append(valid_mae)\n    metrics['valid_loss'].append(valid_loss)\n\n    # Save the model if validation R2 improves\n    if valid_r2 > best_valid_r2:\n        best_valid_r2 = valid_r2\n        torch.save(model.state_dict(), 'best_model.pth')\n        print(f'Saved Best Model at Epoch {epoch + 1} with R2: {valid_r2:.4f}')\n\n# Save metrics to a file\nimport json\nwith open('metrics6.json', 'w') as f:\n    json.dump(metrics, f)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T02:18:48.040052Z","iopub.execute_input":"2024-05-31T02:18:48.040534Z","iopub.status.idle":"2024-05-31T02:19:43.949892Z","shell.execute_reply.started":"2024-05-31T02:18:48.040495Z","shell.execute_reply":"2024-05-31T02:19:43.947736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\n\n# Load the metrics\nwith open('metrics6.json', 'r') as f:\n    metrics = json.load(f)\n\n# Plotting training and validation metrics\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Loss plot\naxes[0, 0].plot(metrics['epoch'], metrics['loss'], label='Train Loss')\naxes[0, 0].plot(metrics['epoch'], metrics['valid_loss'], label='Valid Loss')\naxes[0, 0].set_title('Loss')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].legend()\n\n# MAE plot\naxes[0, 1].plot(metrics['epoch'], metrics['mae'], label='Train MAE')\naxes[0, 1].plot(metrics['epoch'], metrics['valid_mae'], label='Valid MAE')\naxes[0, 1].set_title('Mean Absolute Error (MAE)')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('MAE')\naxes[0, 1].legend()\n\n# R2 plot\naxes[1, 0].plot(metrics['epoch'], metrics['r2'], label='Train R2')\naxes[1, 0].plot(metrics['epoch'], metrics['valid_r2'], label='Valid R2')\naxes[1, 0].set_title('R2 Score')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('R2 Score')\naxes[1, 0].legend()\n\n# Learning rate plot\naxes[1, 1].plot(metrics['epoch'], metrics['lr'], label='Learning Rate')\naxes[1, 1].set_title('Learning Rate')\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('Learning Rate')\naxes[1, 1].legend()\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validate on validation set\nVALID_ROWS = []\nmodel.eval()\n\nfor X_sample_valid, valid_id in tqdm(valid_dataset):\n    with torch.no_grad():\n        y_pred = model(X_sample_valid.unsqueeze(0).to('cuda')).detach().cpu().numpy()\n    \n    y_pred = SCALER.inverse_transform(y_pred).squeeze()\n    row = {'id': valid_id}\n    \n    for k, v in zip(CONFIG.TARGET_COLUMNS, y_pred):\n        if k in LOG_FEATURES:\n            row[k] = 10 ** v\n        else:\n            row[k] = v\n\n    VALID_ROWS.append(row)\n    \nvalid_predict_df = pd.DataFrame(VALID_ROWS)\nprint(valid_predict_df.head())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# valid_y_true\nprint(valid[['id'] + CONFIG.TARGET_COLUMNS].head())\nvalid_y_true = torch.tensor(valid[CONFIG.TARGET_COLUMNS].to_numpy()).to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T02:00:49.061681Z","iopub.execute_input":"2024-05-23T02:00:49.062126Z","iopub.status.idle":"2024-05-23T02:00:49.076088Z","shell.execute_reply.started":"2024-05-23T02:00:49.062094Z","shell.execute_reply":"2024-05-23T02:00:49.074358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate valid scores\nvalid_y_pred = torch.tensor(valid_predict_df[CONFIG.TARGET_COLUMNS].to_numpy()).to('cuda')\n\nwith torch.no_grad():\n    # Calculate R2 Loss\n    print(\"Validation R2 Loss (using r2_loss):\", r2_loss(valid_y_pred, valid_y_true))\n\n    # Loss function (smooth L1 loss)\n    valid_loss = LOSS_FN(valid_y_pred, valid_y_true)\n    print(\"Validation loss (Smooth L1 loss): \", valid_loss)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAE_valid = torchmetrics.regression.MeanAbsoluteError().to('cuda')\nR2_valid = torchmetrics.regression.R2Score(num_outputs=CONFIG.N_TARGETS, multioutput='uniform_average').to('cuda')\n\nprint(\"Torch R2 valid:\", R2_valid(valid_y_pred, valid_y_true))\nprint(\"Torch MAE valid:\", MAE_valid(valid_y_pred, valid_y_true))","metadata":{"execution":{"iopub.status.busy":"2024-05-23T02:23:56.486144Z","iopub.execute_input":"2024-05-23T02:23:56.486743Z","iopub.status.idle":"2024-05-23T02:23:56.554094Z","shell.execute_reply.started":"2024-05-23T02:23:56.486704Z","shell.execute_reply":"2024-05-23T02:23:56.552215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on test set\nSUBMISSION_ROWS = []\nmodel.eval()\n\nfor X_sample_test, test_id in tqdm(test_dataset):\n    with torch.no_grad():\n        y_pred = model(X_sample_test.unsqueeze(0).to('cuda')).detach().cpu().numpy()\n    \n    y_pred = SCALER.inverse_transform(y_pred).squeeze()\n    row = {'id': test_id}\n    \n    for k, v in zip(CONFIG.TARGET_COLUMNS, y_pred):\n        if k in LOG_FEATURES:\n            row[k.replace('_mean', '')] = 10 ** v\n        else:\n            row[k.replace('_mean', '')] = v\n\n    SUBMISSION_ROWS.append(row)\n    \nsubmission_df = pd.DataFrame(SUBMISSION_ROWS)\nprint(submission_df.head())\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Submit!\")","metadata":{"execution":{"iopub.status.busy":"2024-05-14T01:59:25.510075Z","iopub.status.idle":"2024-05-14T01:59:25.510566Z","shell.execute_reply.started":"2024-05-14T01:59:25.510303Z","shell.execute_reply":"2024-05-14T01:59:25.510334Z"},"trusted":true},"execution_count":null,"outputs":[]}]}